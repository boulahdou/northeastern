{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Value Iteration\n",
      "[0, 1, 2, 3, 4, 5, 0]\n",
      "[2.0, 3.0, 4.0, 3, 4, 5, 0]\n",
      "[3.0, 4.0, 4.0, 3.0, 4.0, 5.0, 0.0]\n",
      "[3.5, 4.0, 4.0, 3.0, 4.0, 5.0, 0.0]\n",
      "\n",
      "['PLAY', 'PLAY', 'PLAY', 'STOP', 'STOP', 'STOP', 'STOP']\n",
      "\n",
      "In state 0 the optimal action is PLAY with value 3.5\n",
      "In state 1 the optimal action is PLAY with value 4.0\n",
      "In state 2 the optimal action is PLAY with value 4.0\n",
      "In state 3 the optimal action is STOP with value 3.0\n",
      "In state 4 the optimal action is STOP with value 4.0\n",
      "In state 5 the optimal action is STOP with value 5.0\n",
      "In state DONE the optimal action is STOP with value 0.0\n"
     ]
    }
   ],
   "source": [
    "# Recall our Markov Decision Process (MDP) from last week:\n",
    "# +3 points for Heads, +1 point for Tails\n",
    "# Stop at any time and collect that many points as your payoff.\n",
    "# But if you get 6 or more points, your payoff is nothing.\n",
    "\n",
    "\n",
    "# List our States (S), Actions (A), and Discount Factor (Gamma)\n",
    "S = [0, 1, 2, 3, 4, 5, 'DONE']\n",
    "A = ['STOP', 'PLAY']\n",
    "Gamma = 1\n",
    "\n",
    "\n",
    "# Define the Transition function with three parameters:\n",
    "# current state (s), action (a), new state (n)\n",
    "def T(s,a,n):\n",
    "    if a=='STOP' and n=='DONE':\n",
    "        return 1\n",
    "    if a=='PLAY':\n",
    "        if s=='DONE':\n",
    "            if n=='DONE': return 1\n",
    "        else:\n",
    "            if n=='DONE':\n",
    "                if s==5: return 1\n",
    "                if s==4: return 0.5\n",
    "                if s==3: return 0.5\n",
    "            else:\n",
    "                if n-s == 1: return 0.5\n",
    "                if n-s == 3: return 0.5\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Define the Reward function with three parameters:\n",
    "# current state (s), action (a), new state (n)\n",
    "def R(s,a,n):\n",
    "    if a=='STOP' and n=='DONE':\n",
    "        if s in [0,1,2,3,4,5]: return s\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Define the Value Iteration table V[k][s] and the\n",
    "# Policy Iteration table P[k][s], with 101 rows\n",
    "V = [ [0 for s in S] for k in range(101)]\n",
    "P = [ ['' for s in S] for k in range(101)]\n",
    "\n",
    "\n",
    "# Use the Bellman Equation to determine Row k+1 of\n",
    "# tables V and P, using the information in Row k.\n",
    "\n",
    "for k in range(100):\n",
    "    for s in S:\n",
    "        \n",
    "        # Initialize bestValue and bestPolicy\n",
    "        bestValue=-1\n",
    "        bestPolicy='UNKNOWN'\n",
    "\n",
    "        # For each State-Action pair (s,a), determine Q(s,a) using the\n",
    "        # Bellman Equation by considering all new states n from state s.\n",
    "        \n",
    "        for a in A:\n",
    "            qValue = 0\n",
    "            for n in S:\n",
    "                qValue += T(s,a,n)*(R(s,a,n) + Gamma*V[k][S.index(n)])\n",
    "            if qValue > bestValue:\n",
    "                bestValue = qValue\n",
    "                bestPolicy = a\n",
    "                \n",
    "        V[k+1][S.index(s)]=bestValue\n",
    "        P[k+1][S.index(s)]=bestPolicy\n",
    "        \n",
    "        \n",
    "# Print the results of our Value iteration\n",
    "print(\"Results of Value Iteration\")\n",
    "x=1\n",
    "while V[x] != V[x-1]:\n",
    "    print(V[x])\n",
    "    x+=1\n",
    "    \n",
    "# Print the final (optimal) Policy\n",
    "print(\"\")\n",
    "print(P[x])\n",
    "\n",
    "# Output the results of our 100th iteration.\n",
    "print(\"\")\n",
    "for s in S:\n",
    "    i = S.index(s)\n",
    "    print(\"In state\", s, \"the optimal action is\", P[100][i], \"with value\", V[100][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 [0, 0, 0, 0, 0, 0, 0]\n",
      "State 0 Action PLAY Flip TAILS Updated Value 0.0\n",
      "State 1 Action PLAY Flip TAILS Updated Value 0.0\n",
      "State 2 Action PLAY Flip TAILS Updated Value 0.0\n",
      "State 3 Action STOP Flip NONE Updated Value 1.5\n",
      "State 4 Action STOP Flip NONE Updated Value 2.0\n",
      "State 5 Action STOP Flip NONE Updated Value 2.5\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "\n",
      "1 [0.0, 0.0, 0.0, 1.5, 2.0, 2.5, 0.0]\n",
      "State 0 Action PLAY Flip TAILS Updated Value 0.0\n",
      "State 1 Action PLAY Flip HEADS Updated Value 1.0\n",
      "State 2 Action PLAY Flip TAILS Updated Value 0.75\n",
      "State 3 Action STOP Flip NONE Updated Value 2.25\n",
      "State 4 Action STOP Flip NONE Updated Value 3.0\n",
      "State 5 Action STOP Flip NONE Updated Value 3.75\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "\n",
      "2 [0.0, 1.0, 0.75, 2.25, 3.0, 3.75, 0.0]\n",
      "State 0 Action PLAY Flip TAILS Updated Value 0.5\n",
      "State 1 Action PLAY Flip TAILS Updated Value 0.875\n",
      "State 2 Action PLAY Flip TAILS Updated Value 1.5\n",
      "State 3 Action STOP Flip NONE Updated Value 2.625\n",
      "State 4 Action STOP Flip NONE Updated Value 3.5\n",
      "State 5 Action STOP Flip NONE Updated Value 4.375\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "\n",
      "3 [0.5, 0.875, 1.5, 2.625, 3.5, 4.375, 0.0]\n",
      "State 0 Action PLAY Flip HEADS Updated Value 1.5625\n",
      "State 1 Action PLAY Flip TAILS Updated Value 1.1875\n",
      "State 2 Action PLAY Flip TAILS Updated Value 2.0625\n",
      "State 3 Action STOP Flip NONE Updated Value 2.8125\n",
      "State 4 Action STOP Flip NONE Updated Value 3.75\n",
      "State 5 Action STOP Flip NONE Updated Value 4.6875\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n"
     ]
    }
   ],
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "\n",
    "# Let P be the optimal policy, found from the above MDP.\n",
    "S = [0, 1, 2, 3, 4, 5, 'DONE']\n",
    "P = ['PLAY', 'PLAY', 'PLAY', 'STOP', 'STOP', 'STOP', 'STOP']\n",
    "\n",
    "\n",
    "# Initialize the Values Array V and set a Learning Rate of Alpha=0.5\n",
    "V = [ [0 for s in S] for k in range(1000)]\n",
    "Alpha = 0.5\n",
    "\n",
    "\n",
    "# Run TD Learning\n",
    "\n",
    "import random\n",
    "k = 0\n",
    "for k in range(4):\n",
    "    print(\"\")\n",
    "    print(k, V[k])\n",
    "    \n",
    "    # For each state (s), determine the action (a) based on the optimal policy\n",
    "    # Run an experiement: find the new state (n), calculate the reward, and\n",
    "    # generate the Sample Value.  Use this sample to update V(s)\n",
    "    \n",
    "    for s in S:\n",
    "        \n",
    "        i = S.index(s)\n",
    "        a = P[i]\n",
    "        coin = 'NONE'\n",
    "        \n",
    "        if a == 'STOP':\n",
    "            n = 'DONE'\n",
    "        else:\n",
    "            if random.random() <0.5: \n",
    "                coin = 'TAILS'\n",
    "                n = s+1\n",
    "            else: \n",
    "                coin = 'HEADS'\n",
    "                n = s+3\n",
    "            \n",
    "        j = S.index(n)\n",
    "        sample = R(s,a,n) + V[k][j]\n",
    "\n",
    "        \n",
    "        # The new estimate for V(i) is equal to (1-Alpha) times the old estimate \n",
    "        # for V(i) plus (Alpha) times the result of our sample.  If Alpha = 0.5,\n",
    "        # then these two results have the same weight of 50%.\n",
    "        V[k+1][i]=(1-Alpha)*V[k][i] + Alpha*sample\n",
    "        \n",
    "        print(\"State\", s, \"Action\", a, \"Flip\", coin, \"Updated Value\", V[k+1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['STOP', 'PLAY']\n",
      "[0, 1, 2, 3, 4, 5, 'DONE']\n",
      "\n",
      "0 [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]\n",
      "State 0 Action STOP Flip NONE Updated Value 0.0\n",
      "State 0 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 1 Action STOP Flip NONE Updated Value 0.5\n",
      "State 1 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 2 Action STOP Flip NONE Updated Value 1.0\n",
      "State 2 Action PLAY Flip TAILS Updated Value 0.0\n",
      "State 3 Action STOP Flip NONE Updated Value 1.5\n",
      "State 3 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 4 Action STOP Flip NONE Updated Value 2.0\n",
      "State 4 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 5 Action STOP Flip NONE Updated Value 2.5\n",
      "State 5 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "State DONE Action PLAY Flip NONE Updated Value 0.0\n",
      "\n",
      "1 [[0.0, 0.0], [0.5, 0.0], [1.0, 0.0], [1.5, 0.0], [2.0, 0.0], [2.5, 0.0], [0.0, 0.0]]\n",
      "State 0 Action STOP Flip NONE Updated Value 0.0\n",
      "State 0 Action PLAY Flip HEADS Updated Value 0.75\n",
      "State 1 Action STOP Flip NONE Updated Value 0.75\n",
      "State 1 Action PLAY Flip HEADS Updated Value 1.0\n",
      "State 2 Action STOP Flip NONE Updated Value 1.5\n",
      "State 2 Action PLAY Flip HEADS Updated Value 1.25\n",
      "State 3 Action STOP Flip NONE Updated Value 2.25\n",
      "State 3 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 4 Action STOP Flip NONE Updated Value 3.0\n",
      "State 4 Action PLAY Flip TAILS Updated Value 1.25\n",
      "State 5 Action STOP Flip NONE Updated Value 3.75\n",
      "State 5 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "State DONE Action PLAY Flip NONE Updated Value 0.0\n",
      "\n",
      "2 [[0.0, 0.75], [0.75, 1.0], [1.5, 1.25], [2.25, 0.0], [3.0, 1.25], [3.75, 0.0], [0.0, 0.0]]\n",
      "State 0 Action STOP Flip NONE Updated Value 0.0\n",
      "State 0 Action PLAY Flip HEADS Updated Value 1.5\n",
      "State 1 Action STOP Flip NONE Updated Value 0.875\n",
      "State 1 Action PLAY Flip TAILS Updated Value 1.25\n",
      "State 2 Action STOP Flip NONE Updated Value 1.75\n",
      "State 2 Action PLAY Flip TAILS Updated Value 1.75\n",
      "State 3 Action STOP Flip NONE Updated Value 2.625\n",
      "State 3 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 4 Action STOP Flip NONE Updated Value 3.5\n",
      "State 4 Action PLAY Flip TAILS Updated Value 2.5\n",
      "State 5 Action STOP Flip NONE Updated Value 4.375\n",
      "State 5 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "State DONE Action PLAY Flip NONE Updated Value 0.0\n",
      "\n",
      "3 [[0.0, 1.5], [0.875, 1.25], [1.75, 1.75], [2.625, 0.0], [3.5, 2.5], [4.375, 0.0], [0.0, 0.0]]\n",
      "State 0 Action STOP Flip NONE Updated Value 0.0\n",
      "State 0 Action PLAY Flip HEADS Updated Value 2.0625\n",
      "State 1 Action STOP Flip NONE Updated Value 0.9375\n",
      "State 1 Action PLAY Flip TAILS Updated Value 1.5\n",
      "State 2 Action STOP Flip NONE Updated Value 1.875\n",
      "State 2 Action PLAY Flip HEADS Updated Value 3.0625\n",
      "State 3 Action STOP Flip NONE Updated Value 2.8125\n",
      "State 3 Action PLAY Flip HEADS Updated Value 0.0\n",
      "State 4 Action STOP Flip NONE Updated Value 3.75\n",
      "State 4 Action PLAY Flip TAILS Updated Value 3.4375\n",
      "State 5 Action STOP Flip NONE Updated Value 4.6875\n",
      "State 5 Action PLAY Flip TAILS Updated Value 0.0\n",
      "State DONE Action STOP Flip NONE Updated Value 0.0\n",
      "State DONE Action PLAY Flip NONE Updated Value 0.0\n"
     ]
    }
   ],
   "source": [
    "# Q Learning\n",
    "\n",
    "Q = [ [[0 for a in A] for s in S] for k in range(1000)]\n",
    "\n",
    "k = 0\n",
    "Alpha = 0.5\n",
    "\n",
    "print(A)\n",
    "print(S)\n",
    "\n",
    "for k in range(4):\n",
    "    print(\"\")\n",
    "    print(k, Q[k])\n",
    "    \n",
    "    for s in S:\n",
    "        for a in A:     \n",
    "            coin = 'NONE'\n",
    "            \n",
    "            if a == 'STOP' or s == 'DONE':\n",
    "                n = 'DONE'\n",
    "            else:\n",
    "                if random.random() <0.5: \n",
    "                    coin = 'TAILS'\n",
    "                    n = s+1\n",
    "                    if not n in S: n = 'DONE'\n",
    "                else: \n",
    "                    coin = 'HEADS'\n",
    "                    n = s+3\n",
    "                    if not n in S: n = 'DONE'\n",
    "            \n",
    "            # Get the correct indices for s and n from set S, and the correct\n",
    "            # index for a from A - i.e., A.index('STOP')=0, A.index('PLAY')=1\n",
    "            i = S.index(s)\n",
    "            j = S.index(n)\n",
    "            b = A.index(a)\n",
    "            \n",
    "            # Calculate the sample and update the (k+1)th row of the Q table\n",
    "            \n",
    "            sample = R(s,a,n) + max(Q[k][j][0], Q[k][j][1])\n",
    "            \n",
    "            Q[k+1][i][b] = (1-Alpha)*Q[k][i][b] + Alpha*sample\n",
    "            \n",
    "            print(\"State\", s, \"Action\", a, \"Flip\", coin, \"Updated Value\", Q[k+1][i][b])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
